{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4: Autoencoders and RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import Network as Network\n",
    "import mnist_loader\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (a) Derivative of Cosine Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Loss function is\n",
    "$$\n",
    "C ( \\vec{y} , \\vec{t} ) = \\frac{ - \\left( \\vec{y} \\cdot \\vec{t} \\right) }{ \\| \\vec{y} \\| \\ \\| \\vec{t} \\|}\n",
    "$$\n",
    "We are given that the output layer uses the identity mapping as an activation function, $\\vec{y} = \\vec{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial {y_i}} = - \n",
    "\\frac {{\\left(|\\vec{y}| \\cdot |\\vec{t}| \\right)} \\cdot {t_i} + \n",
    "\\frac {{2y_i} \\cdot { \\|  \\vec{t} \\|}} {2 \\| \\vec{y} \\|} \\cdot \\vec{y} \\cdot \\vec{t}\n",
    "} {\n",
    "\t{\\| \\vec{y} \\| ^2} \t{\\| \\vec{t} \\| ^2} \n",
    "}\n",
    "= - \\frac {t_i} {{\\|\\vec{y}\\|}{\\|\\vec{t}\\|}} - C ( \\vec{y} , \\vec{t} ) \\cdot \\frac {y_i} {\\| \\vec{y} \\| ^2} \n",
    "\\end{align}\n",
    "$$\n",
    "Hence,\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\vec{y}} = - \\frac {t} {{\\|\\vec{y}\\|}{\\|\\vec{t}\\|}} - C ( \\vec{y} , \\vec{t} ) \\cdot \\frac {y} {\\| \\vec{y} \\| ^2} \n",
    "\\end{align}\n",
    "$$  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (b) Implement Derivative of Cosine Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cosine Proximity\n",
    "def CosineProximity(y, t):\n",
    "    '''\n",
    "        C = CosineProximity(y, t)\n",
    "        \n",
    "        Evaluates the average cosine proximity for the batch.\n",
    "        \n",
    "        Inputs:\n",
    "          y is a batch of samples, with samples stored in rows\n",
    "          t is a batch of targets\n",
    "          \n",
    "        Output:\n",
    "          C is the average cosine proximity (cost)\n",
    "    '''\n",
    "    C = -np.sum(y*t, axis=1)\n",
    "    C /= np.linalg.norm(y, axis=1)\n",
    "    C /= np.linalg.norm(t, axis=1)\n",
    "    return np.sum(C) / Network.NSamples(y)\n",
    "\n",
    "\n",
    "# CosineProximity_p\n",
    "def CosineProximity_p(y, t):\n",
    "    '''\n",
    "        dCdy = CosineProximity_p(y, t)\n",
    "        \n",
    "        Computes the gradient of the cosine proximity cost function.\n",
    "        \n",
    "        Inputs:\n",
    "          y is a batch of samples, with samples stored in rows\n",
    "          t is a batch of targets\n",
    "          \n",
    "        Output:\n",
    "          dCdy is an array the same size as y, holding the derivative\n",
    "               of the cost with respect to each element in y\n",
    "    '''\n",
    "\n",
    "    E = CosineProximity(y, t)\n",
    "    y2norm = np.linalg.norm(y, axis=1)\n",
    "    t2norm = np.linalg.norm(t, axis=1)\n",
    "    dCdy = (np.divide(t.T,(y2norm*t2norm).T)).T*(-1)-np.divide(y.T,(y2norm**2).T).T*E\n",
    "    return dCdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (c) Create and Train a Network\n",
    "You can make your network use your cost function and its derivative by setting the member variables,\n",
    "\n",
    "    mynet.cost = CosineProximity\n",
    "    mynet.cost_p = CosineProximity_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read in 10000 MNIST samples\n",
    "train, validate, test = mnist_loader.load_data_wrapper()\n",
    "train_in = np.array(train[0][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some sample digit images\n",
    "plt.figure(figsize=[15,4])\n",
    "n_digits = 4\n",
    "for n in range(n_digits):\n",
    "    idx = np.random.randint(10000)\n",
    "    plt.subplot(2,4,n+1)\n",
    "    plt.imshow(np.reshape(train_in[idx], [28,28]), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89ff8681acaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmynet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'logistic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# train data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmynet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The traing cost is %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/neural_network/A4/Network.py\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, inputs, targets, lrate, epochs, batch_size)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMakeBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeedForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBackProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/neural_network/A4/Network.py\u001b[0m in \u001b[0;36mFeedForward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Calc. input current to next layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# Use activation function to get activities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ***** YOUR CODE HERE *****\n",
    "target = train_in\n",
    "mynet = Network.Network()\n",
    "# input layer\n",
    "mynet.AddLayer(Network.Layer(784)) \n",
    "# hidden layer\n",
    "mynet.AddLayer(Network.Layer(50, act = 'logistic'))\n",
    "# output layer\n",
    "mynet.AddLayer(Network.Layer(784, act = 'logistic'))\n",
    "# train data\n",
    "mynet.SGD(train_in, train_in, batch_size=50, epochs=250, lrate=1.)\n",
    "print(\"The traing cost is %d\" % net.Evaluate(train_in, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (d) View Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ***** YOUR CODE HERE *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial U}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR  $$\\frac{\\partial E}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Q3: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The code below creates two lists:\n",
    "  - `sentences`, and\n",
    "  - `next_chars`\n",
    "  \n",
    "Each list element represents a sequences of characters. There are 3 ways to represent a character:\n",
    "1. As a string, eg. `'b'`\n",
    "2. As an index to a character set, eg. `2`\n",
    "3. As a one-hot vector, eg. `[0, 0, 1, 0, ...]`\n",
    "\n",
    "The lists `sentences` and `next_chars` store the characters as indices (item 2 above). The utility functions\n",
    "  - `char2vec`\n",
    "  - `index2vec`\n",
    "  - `vec2char`\n",
    "  - `vec2index`\n",
    "  \n",
    "transform the characters between the 3 representations. You can also use the dictionaries `char_indices` and `indices_char` to convert between a string character and and index. The code below contains some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character set:  abcdefghijklmnopqrstuvwxyz (first char is a space)\n",
      "There are 27 characters in our character set\n",
      "Here is how you can view one of the samples:\n",
      "Sample input: [on the ori]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = open('origin_of_species.txt').read().lower()\n",
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, \"\\0\") #Add newline character\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "# Let's simplify it by keeping only letters and spaces\n",
    "filt_idx = []\n",
    "for i in idx:\n",
    "    if i<=24:\n",
    "        filt_idx.append(2)\n",
    "    elif i>24:\n",
    "        filt_idx.append(i)\n",
    "blah = ''.join([indices_char[f] for f in filt_idx])\n",
    "text = re.sub(' +', ' ', blah)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Character set: '+''.join(chars)+' (first char is a space)')\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "print('There are '+str(vocab_size)+' characters in our character set')\n",
    "\n",
    "''.join(indices_char[i] for i in idx[:70])\n",
    "\n",
    "def char2vec(c):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[char_indices[c]] = 1.\n",
    "    return v\n",
    "\n",
    "def index2vec(i):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[i] = 1.\n",
    "    return v\n",
    "\n",
    "def vec2index(v):\n",
    "    i = np.argmax(v)\n",
    "    return i\n",
    "\n",
    "def vec2char(v):\n",
    "    return indices_char[vec2index(v)]\n",
    "\n",
    "'''Form the dataset in sentences'''\n",
    "sentences_length = 10\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, 10000 - sentences_length + 1):\n",
    "    sentences.append(idx[i: i + sentences_length]) #Assume a sentence is made of X characters\n",
    "    next_chars.append(idx[i + 1: i + sentences_length + 1]) #Offset by 1 to the right for the target\n",
    "\n",
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])\n",
    "sentences.shape, next_chars.shape\n",
    "\n",
    "def read_sentence(idx):\n",
    "    return ''.join(indices_char[i] for i in idx)\n",
    "\n",
    "print('Here is how you can view one of the samples:')\n",
    "print('Sample input: ['+read_sentence(sentences[0])+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return np.clip(z, a_min=0, a_max=None)  # ReLU\n",
    "    #return 1./(1+np.exp(-z))  # use this for logistic\n",
    "\n",
    "def sigma_primed(y):\n",
    "    return np.clip(np.sign(y), a_min=0, a_max=1)  # Derivative of ReLU\n",
    "    #return y*(1.-y)  # use this for logistic\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    denom = np.sum(ez)\n",
    "    return ez / denom\n",
    "\n",
    "def CrossEntropy(y, t):\n",
    "    return -sum(t*np.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (a) Complete BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, dims, seq_length=10):\n",
    "        '''\n",
    "         Input:\n",
    "           dims is [X, H, Y], where the input has layer has X neurons, the\n",
    "                hidden layer has H neurons, and the output layer has Y neurons.\n",
    "           seq_length is how many steps to unroll the RNN through time\n",
    "                (this is the same as tau in the lecture notes)\n",
    "        '''\n",
    "        self.X, self.H, self.Y = dims\n",
    "        self.seq_length = seq_length\n",
    "        # Input layer\n",
    "        self.xs = [np.zeros(self.X) for x in range(seq_length)] # activity\n",
    "        # Hidden layer\n",
    "        self.hs = [np.zeros(self.H) for x in range(seq_length)] # activity\n",
    "        # Output layer\n",
    "        self.ys = [np.zeros(self.Y) for x in range(seq_length)] # activity\n",
    "        \n",
    "        # Connection weights\n",
    "        self.U = np.random.normal(size=[self.H, self.X])/np.sqrt(self.X) # input->hidden\n",
    "        self.W = np.random.normal(size=[self.H, self.H])/np.sqrt(self.H) # hidden->hidden\n",
    "        self.V = np.random.normal(size=[self.Y, self.H])/np.sqrt(self.H) # hidden->output\n",
    "        self.b = np.zeros(self.H) # biases for hidden nodes\n",
    "        self.c = np.zeros(self.Y) # biases for output nodes\n",
    "        \n",
    "    def ForwardTT(self, seq_in):\n",
    "        '''\n",
    "         i = ForwardTT(seq_in)\n",
    "        \n",
    "         Propagates the RNN forward through time, saving all the intermediate\n",
    "         states that will be needed for backprop through time (BPTT).\n",
    "        \n",
    "         Input:\n",
    "           seq_in is a vector of indecies, with self.seq_length elements.\n",
    "        \n",
    "         Output:\n",
    "           i is the index of the character predicted to follow the input.\n",
    "         \n",
    "         This method's main purpose is to update the states of the activites\n",
    "         in the time-unrolled network.\n",
    "        '''\n",
    "        self.xs[0] = index2vec(seq_in[0]) # convert to character vector\n",
    "        \n",
    "        # Starting input current for hidden nodes\n",
    "        ss = np.dot(self.U, self.xs[0]) + self.b\n",
    "        self.hs[0] = sigma(ss)  # Activation of hidden nodes\n",
    "        \n",
    "        # Input current for output nodes\n",
    "        zs = np.dot(self.V, self.hs[0]) + self.c\n",
    "        self.ys[0] = softmax(zs)  # Activation of output nodes\n",
    "        \n",
    "        # Now process forward in time\n",
    "        for i in range(1, self.seq_length):\n",
    "            self.xs[i] = index2vec(seq_in[i])  # input vector\n",
    "            \n",
    "            # Input current for hidden nodes, including recurrent connections\n",
    "            ss = np.dot(self.U, self.xs[i]) + np.dot(self.W, self.hs[i-1]) + self.b\n",
    "            self.hs[i] = sigma(ss)  # Activation\n",
    "            \n",
    "            # Input current for output nodes\n",
    "            zs = np.dot(self.V, self.hs[i]) + self.c\n",
    "            self.ys[i] = softmax(zs)  # Activation\n",
    "            \n",
    "        # Might as well output the final state of the output\n",
    "        return vec2index(self.ys[-1])\n",
    "    \n",
    "    \n",
    "    def BPTT(self, seq_in, seq_out):\n",
    "        '''\n",
    "         BPTT(seq_in, seq_out)\n",
    "         \n",
    "         Performs backprop through time on one sample given by the input and\n",
    "         output sequence.\n",
    "         \n",
    "         Input:\n",
    "           seq_in is a vector of indices specifying the input sequence of\n",
    "                   characters.\n",
    "           seq_out is a vector of indices specifying the output sequence of\n",
    "                   characters. Typically, seq_out is the same as seq_in, but\n",
    "                   shifted by 1 character.\n",
    "         \n",
    "         Output:\n",
    "           None, but the connection weights and biases are updated.\n",
    "        '''\n",
    "        # Initialize gradients to zero\n",
    "        self.dEdV = np.zeros(np.shape(self.V))\n",
    "        self.dEdW = np.zeros(np.shape(self.W))\n",
    "        self.dEdU = np.zeros(np.shape(self.U))\n",
    "        self.dEdb = np.zeros(np.shape(self.b))\n",
    "        self.dEdc = np.zeros(np.shape(self.c))\n",
    "        \n",
    "        # ===================\n",
    "        # ===================\n",
    "        # =  YOUR CODE HERE =\n",
    "        # ===================\n",
    "        # ===================\n",
    "        \n",
    "            \n",
    "    def Generate(self, n=1):\n",
    "        '''\n",
    "         c = Generate(n=1)\n",
    "         \n",
    "         Runs the RNN from the last state after running ForwardTT, outputting\n",
    "         the next n characters.\n",
    "         \n",
    "         Input:\n",
    "           n is the number of characters you want to predict\n",
    "           \n",
    "         Output:\n",
    "           c is a string of n characters\n",
    "        '''\n",
    "        y = self.ys[-1]  # Final output of ForwardTT\n",
    "        c = vec2char(y)  # Convert it to a character string\n",
    "        h = self.hs[-1]  # Starting with last hidden state...\n",
    "        # Loop forward in time\n",
    "        # (no need to record states, since we won't be doing BPTT)\n",
    "        for nn in range(n-1):\n",
    "            x = copy.copy(y)  # Use last output as next input\n",
    "            \n",
    "            # Input current for hidden nodes\n",
    "            s = np.dot(self.U, x) + np.dot(self.W, h) + self.b\n",
    "            h = sigma(s)  # Activation\n",
    "            \n",
    "            # Input current for output nodes\n",
    "            z = np.dot(self.V, h) + self.c\n",
    "            y = softmax(z)  # Activation\n",
    "            \n",
    "            # And add the next character to our output string\n",
    "            c += vec2char(y)\n",
    "            \n",
    "        return c\n",
    "            \n",
    "    def Evaluate(self, train_in, train_out):\n",
    "        '''\n",
    "         loss = Evaluate(train_in, train_out)\n",
    "         \n",
    "         Evaluates the network on the supplied dataset.\n",
    "         \n",
    "         Input:\n",
    "           train_in is a list of input sequences (see ForwardTT for format of input)\n",
    "           train_out is the corresponding list of output sequences\n",
    "           \n",
    "         Output:\n",
    "           loss is the average cross entropy\n",
    "        '''\n",
    "        val = 0.\n",
    "        for x, t in zip(train_in, train_out):\n",
    "            self.ForwardTT(x)\n",
    "            for i in range(self.seq_length):\n",
    "                val += CrossEntropy(self.ys[i], index2vec(t[i]))\n",
    "        return val/len(train_in)\n",
    "            \n",
    "    def Train(self, train_in, train_out, kappa=0.05, epochs=1):\n",
    "        '''\n",
    "         Train(train_in, train_out, kappa=0.05, epochs=1)\n",
    "         \n",
    "         Performs epochs of gradient descent, performing BPTT after each sample.\n",
    "         \n",
    "         Input:\n",
    "           train_in and train_out is the training dataset\n",
    "           kappa is the learning rate\n",
    "           epochs is the number of times to go through the dataset\n",
    "           \n",
    "         Output:\n",
    "           None, but the connection weights and biases are updated\n",
    "        '''\n",
    "        # Loop over epochs\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            data_shuffled = list(zip(train_in, train_out))\n",
    "            np.random.shuffle(data_shuffled)\n",
    "            \n",
    "            for x, t in data_shuffled:\n",
    "                self.ForwardTT(x)  # Forward through time\n",
    "                self.BPTT(x, t)    # Backprop through time\n",
    "                # Note that BPTT starts by resetting the gradients to zero.\n",
    "                \n",
    "                # Apply update to connection weights and biases\n",
    "                self.V -= kappa*self.dEdV\n",
    "                self.U -= kappa*self.dEdU\n",
    "                self.W -= kappa*self.dEdW\n",
    "                self.b -= kappa*self.dEdb\n",
    "                self.c -= kappa*self.dEdc\n",
    "\n",
    "            print('Epoch '+str(e)+', Loss = '+str(self.Evaluate(train_in, train_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b) Create the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#net = RNN(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# net.Train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You might opt to have more than one train command, using different\n",
    "# learning rates and numbers of epochs. Each one builds on the results\n",
    "# from the last run.\n",
    "#net.Train(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (d) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A sample continuation.\n",
    "n = 38\n",
    "b.ForwardTT(sentences[n])\n",
    "blah = read_sentence(sentences[n])\n",
    "print('Input:      '+blah)\n",
    "print('Prediction: '+blah+b.Generate(5))\n",
    "print('Actual:     '+blah+read_sentence(sentences[n+10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harles dar\n",
      "harles darwin istr c\n"
     ]
    }
   ],
   "source": [
    "blah = 'harles dar'\n",
    "x = [char_indices[c] for c in blah]\n",
    "b.ForwardTT(x)\n",
    "print(blah)\n",
    "print(blah+b.Generate(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
