{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4: Autoencoders and RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0.0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import Network as Network\n",
    "import mnist_loader\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (a) Derivative of Cosine Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Loss function is\n",
    "$$\n",
    "C ( \\vec{y} , \\vec{t} ) = \\frac{ - \\left( \\vec{y} \\cdot \\vec{t} \\right) }{ \\| \\vec{y} \\| \\ \\| \\vec{t} \\|}\n",
    "$$\n",
    "We are given that the output layer uses the identity mapping as an activation function, $\\vec{y} = \\vec{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (b) Implement Derivative of Cosine Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Cosine Proximity\n",
    "def CosineProximity(y, t):\n",
    "    '''\n",
    "        C = CosineProximity(y, t)\n",
    "        \n",
    "        Evaluates the average cosine proximity for the batch.\n",
    "        \n",
    "        Inputs:\n",
    "          y is a batch of samples, with samples stored in rows\n",
    "          t is a batch of targets\n",
    "          \n",
    "        Output:\n",
    "          C is the average cosine proximity (cost)\n",
    "    '''\n",
    "    C = -np.sum(y*t, axis=1)\n",
    "    C /= np.linalg.norm(y, axis=1)\n",
    "    C /= np.linalg.norm(t, axis=1)\n",
    "    return np.sum(C) / Network.NSamples(y)\n",
    "\n",
    "\n",
    "# CosineProximity_p\n",
    "def CosineProximity_p(y, t):\n",
    "    '''\n",
    "        dCdy = CosineProximity_p(y, t)\n",
    "        \n",
    "        Computes the gradient of the cosine proximity cost function.\n",
    "        \n",
    "        Inputs:\n",
    "          y is a batch of samples, with samples stored in rows\n",
    "          t is a batch of targets\n",
    "          \n",
    "        Output:\n",
    "          dCdy is an array the same size as y, holding the derivative\n",
    "               of the cost with respect to each element in y\n",
    "    '''\n",
    "    \n",
    "    # ***** YOUR CODE HERE *****\n",
    "    \n",
    "    dCdy = np.zeros_like(y)  # replace this line of code\n",
    "    \n",
    "    return dCdy\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (c) Create and Train a Network\n",
    "You can make your network use your cost function and its derivative by setting the member variables,\n",
    "\n",
    "    mynet.cost = CosineProximity\n",
    "    mynet.cost_p = CosineProximity_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read in 10000 MNIST samples\n",
    "train, validate, test = mnist_loader.load_data_wrapper()\n",
    "train_in = np.array(train[0][:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxcAAACFCAYAAAApZgXpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f7a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some sample digit images\n",
    "plt.figure(figsize=[15,4])\n",
    "n_digits = 4\n",
    "for n in range(n_digits):\n",
    "    idx = np.random.randint(10000)\n",
    "    plt.subplot(2,n_digits,n+1)\n",
    "    plt.imshow(np.reshape(train_in[idx], [28,28]), cmap='gray'); plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ***** YOUR CODE HERE *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (d) View Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ***** YOUR CODE HERE *****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial U}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR $$\\frac{\\partial E}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "YOUR ANSWER HERE FOR  $$\\frac{\\partial E}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "# Q3: RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The code below creates two lists:\n",
    "  - `sentences`, and\n",
    "  - `next_chars`\n",
    "  \n",
    "Each list element represents a sequences of characters. There are 3 ways to represent a character:\n",
    "1. As a string, eg. `'b'`\n",
    "2. As an index to a character set, eg. `2`\n",
    "3. As a one-hot vector, eg. `[0, 0, 1, 0, ...]`\n",
    "\n",
    "The lists `sentences` and `next_chars` store the characters as indices (item 2 above). The utility functions\n",
    "  - `char2vec`\n",
    "  - `index2vec`\n",
    "  - `vec2char`\n",
    "  - `vec2index`\n",
    "  \n",
    "transform the characters between the 3 representations. You can also use the dictionaries `char_indices` and `indices_char` to convert between a string character and and index. The code below contains some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character set:  abcdefghijklmnopqrstuvwxyz (first char is a space)\n",
      "There are 27 characters in our character set\n",
      "Here is how you can view one of the samples:\n",
      "Sample input: [on the ori]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = open('origin_of_species.txt').read().lower()\n",
    "chars = sorted(list(set(text)))\n",
    "chars.insert(0, \"\\0\") #Add newline character\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "# Let's simplify it by keeping only letters and spaces\n",
    "filt_idx = []\n",
    "for i in idx:\n",
    "    if i<=24:\n",
    "        filt_idx.append(2)\n",
    "    elif i>24:\n",
    "        filt_idx.append(i)\n",
    "blah = ''.join([indices_char[f] for f in filt_idx])\n",
    "text = re.sub(' +', ' ', blah)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Character set: '+''.join(chars)+' (first char is a space)')\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "print('There are '+str(vocab_size)+' characters in our character set')\n",
    "\n",
    "''.join(indices_char[i] for i in idx[:70])\n",
    "\n",
    "def char2vec(c):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[char_indices[c]] = 1.\n",
    "    return v\n",
    "\n",
    "def index2vec(i):\n",
    "    v = np.zeros(vocab_size)\n",
    "    v[i] = 1.\n",
    "    return v\n",
    "\n",
    "def vec2index(v):\n",
    "    i = np.argmax(v)\n",
    "    return i\n",
    "\n",
    "def vec2char(v):\n",
    "    return indices_char[vec2index(v)]\n",
    "\n",
    "'''Form the dataset in sentences'''\n",
    "sentences_length = 10\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, 10000 - sentences_length + 1):\n",
    "    sentences.append(idx[i: i + sentences_length]) #Assume a sentence is made of X characters\n",
    "    next_chars.append(idx[i + 1: i + sentences_length + 1]) #Offset by 1 to the right for the target\n",
    "\n",
    "sentences = np.concatenate([[np.array(o)] for o in sentences[:-2]])\n",
    "next_chars = np.concatenate([[np.array(o)] for o in next_chars[:-2]])\n",
    "sentences.shape, next_chars.shape\n",
    "\n",
    "def read_sentence(idx):\n",
    "    return ''.join(indices_char[i] for i in idx)\n",
    "\n",
    "print('Here is how you can view one of the samples:')\n",
    "print('Sample input: ['+read_sentence(sentences[0])+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return np.clip(z, a_min=0, a_max=None)  # ReLU\n",
    "    #return 1./(1+np.exp(-z))  # use this for logistic\n",
    "\n",
    "def sigma_primed(y):\n",
    "    return np.clip(np.sign(y), a_min=0, a_max=1)  # Derivative of ReLU\n",
    "    #return y*(1.-y)  # use this for logistic\n",
    "\n",
    "def softmax(z):\n",
    "    ez = np.exp(z)\n",
    "    denom = np.sum(ez)\n",
    "    return ez / denom\n",
    "\n",
    "def CrossEntropy(y, t):\n",
    "    return -sum(t*np.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (a) Complete BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    \n",
    "    def __init__(self, dims, seq_length=10):\n",
    "        '''\n",
    "         Input:\n",
    "           dims is [X, H, Y], where the input has layer has X neurons, the\n",
    "                hidden layer has H neurons, and the output layer has Y neurons.\n",
    "           seq_length is how many steps to unroll the RNN through time\n",
    "                (this is the same as tau in the lecture notes)\n",
    "        '''\n",
    "        self.X, self.H, self.Y = dims\n",
    "        self.seq_length = seq_length\n",
    "        # Input layer\n",
    "        self.xs = [np.zeros(self.X) for x in range(seq_length)] # activity\n",
    "        # Hidden layer\n",
    "        self.hs = [np.zeros(self.H) for x in range(seq_length)] # activity\n",
    "        # Output layer\n",
    "        self.ys = [np.zeros(self.Y) for x in range(seq_length)] # activity\n",
    "        \n",
    "        # Connection weights\n",
    "        self.U = np.random.normal(size=[self.H, self.X])/np.sqrt(self.X) # input->hidden\n",
    "        self.W = np.random.normal(size=[self.H, self.H])/np.sqrt(self.H) # hidden->hidden\n",
    "        self.V = np.random.normal(size=[self.Y, self.H])/np.sqrt(self.H) # hidden->output\n",
    "        self.b = np.zeros(self.H) # biases for hidden nodes\n",
    "        self.c = np.zeros(self.Y) # biases for output nodes\n",
    "        \n",
    "    def ForwardTT(self, seq_in):\n",
    "        '''\n",
    "         i = ForwardTT(seq_in)\n",
    "        \n",
    "         Propagates the RNN forward through time, saving all the intermediate\n",
    "         states that will be needed for backprop through time (BPTT).\n",
    "        \n",
    "         Input:\n",
    "           seq_in is a vector of indecies, with self.seq_length elements.\n",
    "        \n",
    "         Output:\n",
    "           i is the index of the character predicted to follow the input.\n",
    "         \n",
    "         This method's main purpose is to update the states of the activites\n",
    "         in the time-unrolled network.\n",
    "        '''\n",
    "        self.xs[0] = index2vec(seq_in[0]) # convert to character vector\n",
    "        \n",
    "        # Starting input current for hidden nodes\n",
    "        ss = np.dot(self.U, self.xs[0]) + self.b\n",
    "        self.hs[0] = sigma(ss)  # Activation of hidden nodes\n",
    "        \n",
    "        # Input current for output nodes\n",
    "        zs = np.dot(self.V, self.hs[0]) + self.c\n",
    "        self.ys[0] = softmax(zs)  # Activation of output nodes\n",
    "        \n",
    "        # Now process forward in time\n",
    "        for i in range(1, self.seq_length):\n",
    "            self.xs[i] = index2vec(seq_in[i])  # input vector\n",
    "            \n",
    "            # Input current for hidden nodes, including recurrent connections\n",
    "            ss = np.dot(self.U, self.xs[i]) + np.dot(self.W, self.hs[i-1]) + self.b\n",
    "            self.hs[i] = sigma(ss)  # Activation\n",
    "            \n",
    "            # Input current for output nodes\n",
    "            zs = np.dot(self.V, self.hs[i]) + self.c\n",
    "            self.ys[i] = softmax(zs)  # Activation\n",
    "            \n",
    "        # Might as well output the final state of the output\n",
    "        return vec2index(self.ys[-1])\n",
    "    \n",
    "    \n",
    "    def BPTT(self, seq_in, seq_out):\n",
    "        '''\n",
    "         BPTT(seq_in, seq_out)\n",
    "         \n",
    "         Performs backprop through time on one sample given by the input and\n",
    "         output sequence.\n",
    "         \n",
    "         Input:\n",
    "           seq_in is a vector of indices specifying the input sequence of\n",
    "                   characters.\n",
    "           seq_out is a vector of indices specifying the output sequence of\n",
    "                   characters. Typically, seq_out is the same as seq_in, but\n",
    "                   shifted by 1 character.\n",
    "         \n",
    "         Output:\n",
    "           None, but the connection weights and biases are updated.\n",
    "        '''\n",
    "        # Initialize gradients to zero\n",
    "        self.dEdV = np.zeros(np.shape(self.V))\n",
    "        self.dEdW = np.zeros(np.shape(self.W))\n",
    "        self.dEdU = np.zeros(np.shape(self.U))\n",
    "        self.dEdb = np.zeros(np.shape(self.b))\n",
    "        self.dEdc = np.zeros(np.shape(self.c))\n",
    "        \n",
    "        # ===================\n",
    "        # ===================\n",
    "        # =  YOUR CODE HERE =\n",
    "        # ===================\n",
    "        # ===================\n",
    "        \n",
    "            \n",
    "    def Generate(self, n=1):\n",
    "        '''\n",
    "         c = Generate(n=1)\n",
    "         \n",
    "         Runs the RNN from the last state after running ForwardTT, outputting\n",
    "         the next n characters.\n",
    "         \n",
    "         Input:\n",
    "           n is the number of characters you want to predict\n",
    "           \n",
    "         Output:\n",
    "           c is a string of n characters\n",
    "        '''\n",
    "        y = self.ys[-1]  # Final output of ForwardTT\n",
    "        c = vec2char(y)  # Convert it to a character string\n",
    "        h = self.hs[-1]  # Starting with last hidden state...\n",
    "        # Loop forward in time\n",
    "        # (no need to record states, since we won't be doing BPTT)\n",
    "        for nn in range(n-1):\n",
    "            x = copy.copy(y)  # Use last output as next input\n",
    "            \n",
    "            # Input current for hidden nodes\n",
    "            s = np.dot(self.U, x) + np.dot(self.W, h) + self.b\n",
    "            h = sigma(s)  # Activation\n",
    "            \n",
    "            # Input current for output nodes\n",
    "            z = np.dot(self.V, h) + self.c\n",
    "            y = softmax(z)  # Activation\n",
    "            \n",
    "            # And add the next character to our output string\n",
    "            c += vec2char(y)\n",
    "            \n",
    "        return c\n",
    "            \n",
    "    def Evaluate(self, train_in, train_out):\n",
    "        '''\n",
    "         loss = Evaluate(train_in, train_out)\n",
    "         \n",
    "         Evaluates the network on the supplied dataset.\n",
    "         \n",
    "         Input:\n",
    "           train_in is a list of input sequences (see ForwardTT for format of input)\n",
    "           train_out is the corresponding list of output sequences\n",
    "           \n",
    "         Output:\n",
    "           loss is the average cross entropy\n",
    "        '''\n",
    "        val = 0.\n",
    "        for x, t in zip(train_in, train_out):\n",
    "            self.ForwardTT(x)\n",
    "            for i in range(self.seq_length):\n",
    "                val += CrossEntropy(self.ys[i], index2vec(t[i]))\n",
    "        return val/len(train_in)\n",
    "            \n",
    "    def Train(self, train_in, train_out, kappa=0.05, epochs=1):\n",
    "        '''\n",
    "         Train(train_in, train_out, kappa=0.05, epochs=1)\n",
    "         \n",
    "         Performs epochs of gradient descent, performing BPTT after each sample.\n",
    "         \n",
    "         Input:\n",
    "           train_in and train_out is the training dataset\n",
    "           kappa is the learning rate\n",
    "           epochs is the number of times to go through the dataset\n",
    "           \n",
    "         Output:\n",
    "           None, but the connection weights and biases are updated\n",
    "        '''\n",
    "        # Loop over epochs\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            # Shuffle the training data\n",
    "            data_shuffled = list(zip(train_in, train_out))\n",
    "            np.random.shuffle(data_shuffled)\n",
    "            \n",
    "            for x, t in data_shuffled:\n",
    "                self.ForwardTT(x)  # Forward through time\n",
    "                self.BPTT(x, t)    # Backprop through time\n",
    "                # Note that BPTT starts by resetting the gradients to zero.\n",
    "                \n",
    "                # Apply update to connection weights and biases\n",
    "                self.V -= kappa*self.dEdV\n",
    "                self.U -= kappa*self.dEdU\n",
    "                self.W -= kappa*self.dEdW\n",
    "                self.b -= kappa*self.dEdb\n",
    "                self.c -= kappa*self.dEdc\n",
    "\n",
    "            print('Epoch '+str(e)+', Loss = '+str(self.Evaluate(train_in, train_out)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (b) Create the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#net = RNN(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## (c) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# net.Train(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# You might opt to have more than one train command, using different\n",
    "# learning rates and numbers of epochs. Each one builds on the results\n",
    "# from the last run.\n",
    "#net.Train(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## (d) Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A sample continuation.\n",
    "n = 38\n",
    "b.ForwardTT(sentences[n])\n",
    "blah = read_sentence(sentences[n])\n",
    "print('Input:      '+blah)\n",
    "print('Prediction: '+blah+b.Generate(5))\n",
    "print('Actual:     '+blah+read_sentence(sentences[n+10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harles dar\n",
      "harles darwin istr c\n"
     ]
    }
   ],
   "source": [
    "blah = 'harles dar'\n",
    "x = [char_indices[c] for c in blah]\n",
    "b.ForwardTT(x)\n",
    "print(blah)\n",
    "print(blah+b.Generate(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
